{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Fake News Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of Contents:**\n",
    "\n",
    "- Introduction\n",
    "- Traditional feature engineering\n",
    "    - Bag of Words model\n",
    "    - Bag of N-Grams model\n",
    "    - TF-IDF model\n",
    "- Document embeddings from pre-trained word embeddings\n",
    "    - Pre-trained Word2Vec\n",
    "    - Pre-trained FastText\n",
    "- Document embeddings from self-trained word embeddings\n",
    "    - Word2Vec\n",
    "    - FastText\n",
    "- Document embeddings from BERT sentence embeddings\n",
    "- Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project I am going to classify news articles into fake news articles and real news articles, based on the linguistic features of the text. In order to classify these articles, I will use different feature engineering techniques, such as the Bag of Words Model, the TF-IDF Model, and document embeddings resulting from several word embedding techniques and the BERT sentence embedding model. \n",
    "\n",
    "This project's main goal is to compare several feature engineering methods. Therefore, I choose to use only one machine learnings algorithm to classify the news articles, which is logistic regression. I tried some others as well, but logistic regression performs well and does not require any hyperparameter tuning (although one could say that regularization could be used as a hyperparameter). \n",
    "\n",
    "The data can be downloaded from [here](https://drive.google.com/file/d/1er9NJTLUA3qnRuyhfzuN0XUsoIC4a-_q/view). Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"D:/Projects/fake news\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import FastText\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the dataset\n",
    "data = pd.read_csv(\"news.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8476</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10294</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3608</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10142</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>875</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0        8476                       You Can Smell Hillary’s Fear   \n",
       "1       10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2        3608        Kerry to go to Paris in gesture of sympathy   \n",
       "3       10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4         875   The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                text label  \n",
       "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n",
       "2  U.S. Secretary of State John F. Kerry said Mon...  REAL  \n",
       "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n",
       "4  It's primary day in New York and front-runners...  REAL  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 6335 columns\n"
     ]
    }
   ],
   "source": [
    "print(\"The dataset contains {} columns\".format(data.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of 6335 news articles. However, some news articles are empty, so let's delete those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove empty articles\n",
    "data = data[data[\"text\"] != \" \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset now consists of 6299 columns\n"
     ]
    }
   ],
   "source": [
    "print(\"The dataset now consists of {} columns\".format(data.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset now consists of 6299 columns. However, there are some duplicate articles in the dataset. Let's delete those duplicates (keep the first article, drop the others)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates\n",
    "data = data.drop_duplicates(subset=[\"text\"], keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset now consists of 6059 columns\n"
     ]
    }
   ],
   "source": [
    "print(\"The dataset now consists of {} columns\".format(data.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FAKE    3070\n",
       "REAL    2989\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of fake and real articles\n",
    "data[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the dataset is balanced. That is, the proportion of real articles and the proportion of fake articles are about 50 percent. For training, this is a desirable feature. It also means that we could use accuracy as the performance metric. Another reason for the use of accuracy as the performance metric, is that misclassifying fake news articles is not more or less important than misclassifying real articles. That is, our business goal is to predict accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Daniel Greenfield, a Shillman Journalism Fellow at the Freedom Center, is a New York writer focusing on radical Islam. \\nIn the final stretch of the election, Hillary Rodham Clinton has gone to war with the FBI. \\nThe word “unprecedented” has been thrown around so often this election that it ought to be retired. But it’s still unprecedented for the nominee of a major political party to go war with the FBI. \\nBut that’s exactly what Hillary and her people have done. Coma patients just waking up now and watching an hour of CNN from their hospital beds would assume that FBI Director James Comey is Hillary’s opponent in this election. \\nThe FBI is under attack by everyone from Obama to CNN. Hillary’s people have circulated a letter attacking Comey. There are currently more media hit pieces lambasting him than targeting Trump. It wouldn’t be too surprising if the Clintons or their allies were to start running attack ads against the FBI. \\nThe FBI’s leadership is being warned that the entire left-wing establishment will form a lynch mob if they continue going after Hillary. And the FBI’s credibility is being attacked by the media and the Democrats to preemptively head off the results of the investigation of the Clinton Foundation and Hillary Clinton. \\nThe covert struggle between FBI agents and Obama’s DOJ people has gone explosively public. \\nThe New York Times has compared Comey to J. Edgar Hoover. Its bizarre headline, “James Comey Role Recalls Hoover’s FBI, Fairly or Not” practically admits up front that it’s spouting nonsense. The Boston Globe has published a column calling for Comey’s resignation. Not to be outdone, Time has an editorial claiming that the scandal is really an attack on all women. \\nJames Carville appeared on MSNBC to remind everyone that he was still alive and insane. He accused Comey of coordinating with House Republicans and the KGB. And you thought the “vast right wing conspiracy” was a stretch. \\nCountless media stories charge Comey with violating procedure. Do you know what’s a procedural violation? Emailing classified information stored on your bathroom server. \\nSenator Harry Reid has sent Comey a letter accusing him of violating the Hatch Act. The Hatch Act is a nice idea that has as much relevance in the age of Obama as the Tenth Amendment. But the cable news spectrum quickly filled with media hacks glancing at the Wikipedia article on the Hatch Act under the table while accusing the FBI director of one of the most awkward conspiracies against Hillary ever. \\nIf James Comey is really out to hurt Hillary, he picked one hell of a strange way to do it. \\nNot too long ago Democrats were breathing a sigh of relief when he gave Hillary Clinton a pass in a prominent public statement. If he really were out to elect Trump by keeping the email scandal going, why did he trash the investigation? Was he on the payroll of House Republicans and the KGB back then and playing it coy or was it a sudden development where Vladimir Putin and Paul Ryan talked him into taking a look at Anthony Weiner’s computer? \\nEither Comey is the most cunning FBI director that ever lived or he’s just awkwardly trying to navigate a political mess that has trapped him between a DOJ leadership whose political futures are tied to Hillary’s victory and his own bureau whose apolitical agents just want to be allowed to do their jobs. \\nThe only truly mysterious thing is why Hillary and her associates decided to go to war with a respected Federal agency. Most Americans like the FBI while Hillary Clinton enjoys a 60% unfavorable rating. \\nAnd it’s an interesting question. \\nHillary’s old strategy was to lie and deny that the FBI even had a criminal investigation underway. Instead her associates insisted that it was a security review. The FBI corrected her and she shrugged it off. But the old breezy denial approach has given way to a savage assault on the FBI. \\nPretending that nothing was wrong was a bad strategy, but it was a better one that picking a fight with the FBI while lunatic Clinton associates try to claim that the FBI is really the KGB. \\nThere are two possible explanations. \\nHillary Clinton might be arrogant enough to lash out at the FBI now that she believes that victory is near. The same kind of hubris that led her to plan her victory fireworks display could lead her to declare a war on the FBI for irritating her during the final miles of her campaign. \\nBut the other explanation is that her people panicked. \\nGoing to war with the FBI is not the behavior of a smart and focused presidential campaign. It’s an act of desperation. When a presidential candidate decides that her only option is to try and destroy the credibility of the FBI, that’s not hubris, it’s fear of what the FBI might be about to reveal about her. \\nDuring the original FBI investigation, Hillary Clinton was confident that she could ride it out. And she had good reason for believing that. But that Hillary Clinton is gone. In her place is a paranoid wreck. Within a short space of time the “positive” Clinton campaign promising to unite the country has been replaced by a desperate and flailing operation that has focused all its energy on fighting the FBI. \\nThere’s only one reason for such bizarre behavior. \\nThe Clinton campaign has decided that an FBI investigation of the latest batch of emails poses a threat to its survival. And so it’s gone all in on fighting the FBI. It’s an unprecedented step born of fear. It’s hard to know whether that fear is justified. But the existence of that fear already tells us a whole lot. \\nClinton loyalists rigged the old investigation. They knew the outcome ahead of time as well as they knew the debate questions. Now suddenly they are no longer in control. And they are afraid. \\nYou can smell the fear. \\nThe FBI has wiretaps from the investigation of the Clinton Foundation. It’s finding new emails all the time. And Clintonworld panicked. The spinmeisters of Clintonworld have claimed that the email scandal is just so much smoke without fire. All that’s here is the appearance of impropriety without any of the substance. But this isn’t how you react to smoke. It’s how you respond to a fire. \\nThe misguided assault on the FBI tells us that Hillary Clinton and her allies are afraid of a revelation bigger than the fundamental illegality of her email setup. The email setup was a preemptive cover up. The Clinton campaign has panicked badly out of the belief, right or wrong, that whatever crime the illegal setup was meant to cover up is at risk of being exposed. \\nThe Clintons have weathered countless scandals over the years. Whatever they are protecting this time around is bigger than the usual corruption, bribery, sexual assaults and abuses of power that have followed them around throughout the years. This is bigger and more damaging than any of the allegations that have already come out. And they don’t want FBI investigators anywhere near it. \\nThe campaign against Comey is pure intimidation. It’s also a warning. Any senior FBI people who value their careers are being warned to stay away. The Democrats are closing ranks around their nominee against the FBI. It’s an ugly and unprecedented scene. It may also be their last stand. \\nHillary Clinton has awkwardly wound her way through numerous scandals in just this election cycle. But she’s never shown fear or desperation before. Now that has changed. Whatever she is afraid of, it lies buried in her emails with Huma Abedin. And it can bring her down like nothing else has.  '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first news article\n",
    "data[\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only need the text column for generating the features (X), and we need the label column for our target variable (y)\n",
    "y = data[\"label\"]\n",
    "X = data[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into a training set and a test set. \n",
    "# We will use the test set to evaluate the performance of the feature engineering techniques. \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_articles_list = X_train.to_list()\n",
    "test_articles_list = X_test.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For most of the feature engineering methods, we are going to use normalized text data. For this we need to take several pre-processing steps. These pre-processing steps include:\n",
    "- Transform the token to the lemma of the word\n",
    "- Get rid of stop words\n",
    "- Get rid of puntcation marks\n",
    "- Get rid of digits\n",
    "- Everything to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a spacy nlp model for text pre-processing\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a normalize function for pre-processing the texts\n",
    "def normalize(article):\n",
    "    article = nlp(article)\n",
    "    # transform to lemma and get rid of stopwords and punctuation marks\n",
    "    article = [token.lemma_ for token in article if not (token.is_stop or token.is_punct)]\n",
    "    # get rid of white spaces\n",
    "    article = [token for token in article if not token.strip() == \"\"]\n",
    "    # get rid of numbers\n",
    "    article = [token for token in article if not re.search(\"[0-9]\", token)]\n",
    "    # everything to lowercase\n",
    "    article = [token.lower() for token in article]\n",
    "    article = \" \".join(article)\n",
    "    return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processing the texts\n",
    "train_articles_normalized = [normalize(article) for article in train_articles_list]\n",
    "test_articles_normalized = [normalize(article) for article in test_articles_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"des moines iowa cnn doubt donald trump pull major counter program feat compete gop debate expect draw million viewer thursday night dazzle crowd hundred enthusiastic supporter announce raise $ million veteran day $ million checkbook love vet say know theme america great go vet trump say trump bite surprise pull stunt look camera like academy awards real estate magnate say take stage auditorium drake university minute debate begin mile away actually tell camera bite know honor vet rally restrain performance trump standard dispense usual riff poll number avoid jab fellow candidate exception low energy shoot jeb bush instead deliver speech focus problem veteran face return iraq afghanistan inadequate healthcare house drug abuse mental health issue homelessness vet mistreat illegal immigrant treat well case vet go happen go happen clearly enjoy even away debate trump tell audience medium sensation campaign fact daughter ivanka pregnant ivanka say great baby iowa great definitely win somewhat extraordinary revel taunt rival invite mike huckabee rick santorum candidate relegate early undercard debate join stage speak veteran issue appear generous politically savvy maneuver give man respective winner past iowa caucus stick tier time huckabee santorum like admire core republican voter iowa campaign fail ignite time presence stage trump long way negate criticism trump rival like ted cruz ally claim trump punish iowans monday skip debate stage santorum narrowly defeat mitt romney try stand trump podium note laughter want photograph trump sign support candidate mean work honor america veteran santorum say trump regale medium spectacle create past day withdraw fox news debate complaint mistreat network tell crowd wish able participate withdraw cajole like fox news host bill o'reilly bring treat badly stick right say cheer country stick country mistreat start vet add reflect pundit right maneuver damage campaign know personally good thing bad thing vote vote know hell know predict money raise website personal call wealthy friend contribute cause impress iowans think money go continue pour organization run gamut group focus help veteran disability mental health problem aim help veteran reintegrate civilian society trump supporter wait hour cold roundly dispute notion attrition support iowa lead recent poll interview voter say controversy example trump buck establishment trait endear begin proud stand fox news ernie ratcliffe army veteran serve tour vietnam drive kansas city rally scoff ask thought texas sen. ted cruz contention trump skip debate afraid taunt difficult question fox moderator rival candidate donald trump scare scare absolutely say ratcliffe sign wife new hampshire voter trump behalf week donald j. trump say go man word ratcliffe say convince trump candidate clean department veterans affairs thing get office go square away say go long go right people know manage thing successful businessman go quickly randal thom marine admit trump event say love trump refuse come yesterday actually rally hour amaze thom say show ability rally thing thom raise alaskan malamute pomalute puppy minnesota plan spend monday iowa volunteer trump dismiss cruz canadian bear citizen describe texas senator gop contender weak trump strongman bullet proof thom say people oh look background look numb wife know care care future\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalized first news article in training set\n",
    "train_articles_normalized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_articles_normalized = np.array(train_articles_normalized, dtype=\"object\")\n",
    "test_articles_normalized = np.array(test_articles_normalized, dtype=\"object\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I am going to use several traditional feature engineering methods in order to classify fake news articles from real articles. These methods are all bag of words models, which literally means that each article is represented as a bag of words, eliminating the word order. These models leads to sparse article vectors as the size of the vector is the size of the resulting dictionary, and the article only contains a limited amount of words. As simple as this approach might be, the resulting prediction accuracy is very good, as we will see!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the standard Bag of Words model will be used. For each article, this method will create a numerical vector, where the size of the vector is equal to the size of the vocabulary, and where each element represents the count of a word that is in the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the count vectorizor\n",
    "count_vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the count vectorizor on the normalized training articles and transform\n",
    "cv_train = count_vectorizer.fit_transform(train_articles_normalized)\n",
    "# transform on the normalized test articles\n",
    "cv_test = count_vectorizer.transform(test_articles_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on the test set is 0.905940594059406\n"
     ]
    }
   ],
   "source": [
    "# instantiate logistic regression model and fit on the training data\n",
    "logreg_cv = LogisticRegression(max_iter=300)\n",
    "logreg_cv.fit(cv_train, y_train)\n",
    "# Predict on test set\n",
    "predictions_logreg_cv = logreg_cv.predict(cv_test)\n",
    "print(\"The accuracy on the test set is {}\".format(accuracy_score(predictions_logreg_cv, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the accuracy on the unseen data (the test set) is about 90.6 percent. This is very high, as a naive prediction model would have an accuracy of about 50 percent. One explanation for why this simple model seems to work very well is that there are certain words that are more used in fake articles than in real articles, and vice versa. \n",
    "\n",
    "Let's see which words are most associated with fake articles, and which words are more associated with real articles. For this we look at the coefficients of the logistic regression model and calculate the odds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = count_vectorizer.get_feature_names()\n",
    "cv_odds = pd.DataFrame({\"words\":vocabulary, \"odds\": np.exp(logreg_cv.coef_[0])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>odds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14378</th>\n",
       "      <td>executive</td>\n",
       "      <td>2.202659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37731</th>\n",
       "      <td>saturday</td>\n",
       "      <td>2.047612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8936</th>\n",
       "      <td>convention</td>\n",
       "      <td>1.977184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38473</th>\n",
       "      <td>sen</td>\n",
       "      <td>1.972079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34739</th>\n",
       "      <td>race</td>\n",
       "      <td>1.963441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16320</th>\n",
       "      <td>friday</td>\n",
       "      <td>1.954766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43810</th>\n",
       "      <td>transition</td>\n",
       "      <td>1.941896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8699</th>\n",
       "      <td>conservative</td>\n",
       "      <td>1.939244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16114</th>\n",
       "      <td>fox</td>\n",
       "      <td>1.931455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26406</th>\n",
       "      <td>marriage</td>\n",
       "      <td>1.892626</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              words      odds\n",
       "14378     executive  2.202659\n",
       "37731      saturday  2.047612\n",
       "8936     convention  1.977184\n",
       "38473           sen  1.972079\n",
       "34739          race  1.963441\n",
       "16320        friday  1.954766\n",
       "43810    transition  1.941896\n",
       "8699   conservative  1.939244\n",
       "16114           fox  1.931455\n",
       "26406      marriage  1.892626"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the 10 words most associated with fake news\n",
    "cv_odds.sort_values(by=\"odds\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>odds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30365</th>\n",
       "      <td>october</td>\n",
       "      <td>0.191716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29984</th>\n",
       "      <td>november</td>\n",
       "      <td>0.366860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38834</th>\n",
       "      <td>share</td>\n",
       "      <td>0.417894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14038</th>\n",
       "      <td>establishment</td>\n",
       "      <td>0.442641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2431</th>\n",
       "      <td>article</td>\n",
       "      <td>0.448705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40244</th>\n",
       "      <td>source</td>\n",
       "      <td>0.452321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42010</th>\n",
       "      <td>swipe</td>\n",
       "      <td>0.486274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33772</th>\n",
       "      <td>print</td>\n",
       "      <td>0.490359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33304</th>\n",
       "      <td>posted</td>\n",
       "      <td>0.500167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30361</th>\n",
       "      <td>oct</td>\n",
       "      <td>0.529232</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               words      odds\n",
       "30365        october  0.191716\n",
       "29984       november  0.366860\n",
       "38834          share  0.417894\n",
       "14038  establishment  0.442641\n",
       "2431         article  0.448705\n",
       "40244         source  0.452321\n",
       "42010          swipe  0.486274\n",
       "33772          print  0.490359\n",
       "33304         posted  0.500167\n",
       "30361            oct  0.529232"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the 10 words least associated with fake news\n",
    "cv_odds.sort_values(by=\"odds\", ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, some words are more associated to fake news than others. The word \"executive\" is most associated with fake news. For every time that \"executive\" occurs in an article, the odds that the observation is in the category fake news are 2.2 times as large as the odds that the observation is not in the category fake news, when all other variables are held constant. Although this supplementary analysis could be very interesting, it is probably the combination of several words in an article that determine whether the article is fake news or real news."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of N-Grams model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see whether we can improve on the standard Bag of Words model, by including bi-grams in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the count vectorizor with unigrams and bi-grams\n",
    "count_vectorizer2 = CountVectorizer(ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the count vectorizor on the normalized training articles and transform\n",
    "cv2_train = count_vectorizer2.fit_transform(train_articles_normalized)\n",
    "# transform on the normalized test articles\n",
    "cv2_test = count_vectorizer2.transform(test_articles_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on the test set is 0.9183168316831684\n"
     ]
    }
   ],
   "source": [
    "# instantiate logistic regression model and fit on the training data\n",
    "logreg_cv2 = LogisticRegression(max_iter=300)\n",
    "logreg_cv2.fit(cv2_train, y_train)\n",
    "# Predict on test set\n",
    "predictions_logreg_cv2 = logreg_cv2.predict(cv2_test)\n",
    "print(\"The accuracy on the test set is {}\".format(accuracy_score(predictions_logreg_cv2, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Including bi-grams in the standard Bag of Words model slightly improves the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple counting the words in the standard Bag of Words model might not be the best way to go. It could be better to use TF-IDF (term frequency - inverse docucment frequency). Let's see how this method performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_train = tfidf_vectorizer.fit_transform(train_articles_normalized)\n",
    "tfidf_test = tfidf_vectorizer.transform(test_articles_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on the test set is 0.9075907590759076\n"
     ]
    }
   ],
   "source": [
    "logreg_tfidf = LogisticRegression(max_iter=300)\n",
    "logreg_tfidf.fit(tfidf_train, y_train)\n",
    "predictions_logreg_tfidf = logreg_tfidf.predict(tfidf_test)\n",
    "print(\"The accuracy on the test set is {}\".format(accuracy_score(predictions_logreg_tfidf, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy for the TF-IDF model is slightly higher then for the standard Bag of Words model. Tuning some of the parameters can lead to slightly different performances. Overall, the traditional feature engineering combined with logistic regression do a very good job in predicting whether an article is fake news or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document embeddings from pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural language processing is in constant development, and one of the new popular methods for feature engineering are word embeddings. Word embeddings are vector representations of words and are trained with neural networks. The idea behind the approach is that if a word occurs in the same context as another word, then these words are similar to one another and have similar meanings. \n",
    "\n",
    "In this section I will use pre-trained word embeddings. There are two models that I will use: Word2Vec and FastText. From these word embeddings I create document embeddings by simply averaging the word embeddings for the respective article. Let's see how these document embeddings perform in classifying articles into the fake news and the real news categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-trained Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading model\n",
    "word2vec_path = \"D:/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz\"\n",
    "model = KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A word embedding vector has 300 elements\n"
     ]
    }
   ],
   "source": [
    "feature_size = len(model[\"fake\"])\n",
    "print(\"A word embedding vector has {} elements\".format(feature_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each word embedding has 300 elements. The smaller the distance between two vectors, the more similar these words are. Let's see which word is most similar to the set of words \"fake news\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bogus', 0.5605027675628662),\n",
       " ('phony', 0.5478013753890991),\n",
       " ('site_FreakingNews.com', 0.5443505644798279),\n",
       " ('MCOT_online', 0.48262903094291687),\n",
       " ('Latest_Tanker_Operator', 0.47789376974105835)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The most similar words to fake news\n",
    "model.most_similar(positive=[\"fake\", \"news\"])[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The smallest distance occurs for the word \"bogus\". Hence, \"bogus\" is most similar to the set of words \"fake news\". Now that we saw that these word embeddings actually capture something relevant, let's calculate document embeddings by simply aggregating the word embeddings for each article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that aggregates the word embeddings into a document embedding\n",
    "def get_document_embeddings(documents, model):\n",
    "    \n",
    "    document_embeddings=[]\n",
    "    for document in documents:\n",
    "        document_embedding = np.zeros((feature_size), dtype=\"float32\")\n",
    "        n_tokens = 0\n",
    "        for token in document:\n",
    "             if token in model:\n",
    "                document_embedding = np.add(document_embedding, model[token])\n",
    "                n_tokens += 1\n",
    "\n",
    "        if n_tokens > 0:\n",
    "            document_embedding = np.divide(document_embedding, n_tokens)\n",
    "\n",
    "        document_embeddings.append(document_embedding)\n",
    "    return document_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create document embeddings for the training and test set\n",
    "train_document_embeddings = get_document_embeddings(train_articles_normalized, model)\n",
    "test_document_embeddings = get_document_embeddings(test_articles_normalized, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on the test set is 0.6468646864686468\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a logistic regression model and fit on training data\n",
    "logreg_word2vec_pretrained = LogisticRegression(max_iter=300)\n",
    "logreg_word2vec_pretrained.fit(train_document_embeddings, y_train)\n",
    "# Generate predictions on the test data\n",
    "predictions_logreg_word2vec_pretrained = logreg_word2vec_pretrained.predict(test_document_embeddings)\n",
    "print(\"The accuracy on the test set is {}\".format(accuracy_score(predictions_logreg_word2vec_pretrained, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the accuracy is higher than the naive prediction model accuracy of about 50 percent, the accuracy of these pre-trained Word2Vec word embeddings is much lower compared with the accuracy of the traditional feature engineering methods. Let's see whether FastText performs better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-trained FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading model\n",
    "fasttext_path = \"D:/gensim-data/fasttext-wiki-news-subwords-300/fasttext-wiki-news-subwords-300.gz\"\n",
    "model = KeyedVectors.load_word2vec_format(fasttext_path, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A word embedding vector has 300 elements\n"
     ]
    }
   ],
   "source": [
    "feature_size = len(model[\"fake\"])\n",
    "print(\"A word embedding vector has {} elements\".format(feature_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fake-news', 0.7599742412567139),\n",
       " ('pseudo-news', 0.7420461773872375),\n",
       " ('good-news', 0.7235771417617798),\n",
       " ('fakey', 0.7220522165298462),\n",
       " ('bad-news', 0.7200027704238892)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The most similar words to fake news\n",
    "model.most_similar(positive=[\"fake\", \"news\"])[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the FastText model has different similar words to \"fake news\" compared with the Word2Vec model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create document embeddings for the training and test set\n",
    "train_document_embeddings = get_document_embeddings(train_articles_normalized, model)\n",
    "test_document_embeddings = get_document_embeddings(test_articles_normalized, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on the test set is 0.6204620462046204\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a logistic regression model and fit on training data\n",
    "logreg_fasttext_pretrained = LogisticRegression(max_iter=300)\n",
    "logreg_fasttext_pretrained.fit(train_document_embeddings, y_train)\n",
    "# Generate predictions on the test data\n",
    "predictions_logreg_fasttext_pretrained = logreg_fasttext_pretrained.predict(test_document_embeddings)\n",
    "print(\"The accuracy on the test set is {}\".format(accuracy_score(predictions_logreg_fasttext_pretrained, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use of FastText word embeddings leads to a lower accuracy compared to the use of Word2Vec embeddings. This is somewhat unexpected, as FastText is a newer model. Nevertheless, in this use case, traditional feature engineering methods outperform the pre-trained word embedding methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document embeddings from self-trained word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I am going to create my own word embeddings, and see whether these perform better or worse compared with the pre-trained word embeddings. I will use the Gensim module to create these word embeddings. Again I will use the Word2Vec model and the FastText model. The size of the embeddings is treated as a hyperparameter that is tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new normalize function, as the Gensim models desire a different data structure as input\n",
    "# More specifically, Gensim models demand context words, and therefore the input should consists of sentences\n",
    "def normalize2(article):\n",
    "    article_doc = nlp(article)\n",
    "    sentences = list(article_doc.sents)\n",
    "    article_normalized = []\n",
    "    for sentence in sentences:\n",
    "        # transform to lemma and get rid of stopwords and punctuation marks\n",
    "        sentence = [token.lemma_ for token in sentence if not (token.is_stop or token.is_punct)]\n",
    "        # get rid of white spaces\n",
    "        sentence = [token for token in sentence if not token.strip() == \"\"]\n",
    "        # get rid of numbers\n",
    "        sentence = [token for token in sentence if not re.search(\"[0-9]\", token)]\n",
    "        # everything to lowercase\n",
    "        sentence= [token.lower() for token in sentence]\n",
    "        article_normalized.append(sentence)\n",
    "    return article_normalized\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get normalized articles\n",
    "train_articles_normalized2 = [normalize2(article) for article in train_articles_list]\n",
    "test_articles_normalized2 = [normalize2(article) for article in test_articles_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['des', 'moines', 'iowa', 'cnn'],\n",
       " ['doubt',\n",
       "  'donald',\n",
       "  'trump',\n",
       "  'pull',\n",
       "  'major',\n",
       "  'counter',\n",
       "  'program',\n",
       "  'feat',\n",
       "  'compete',\n",
       "  'gop',\n",
       "  'debate',\n",
       "  'expect',\n",
       "  'draw',\n",
       "  'million',\n",
       "  'viewer'],\n",
       " ['thursday',\n",
       "  'night',\n",
       "  'dazzle',\n",
       "  'crowd',\n",
       "  'hundred',\n",
       "  'enthusiastic',\n",
       "  'supporter',\n",
       "  'announce',\n",
       "  'raise',\n",
       "  '$',\n",
       "  'million',\n",
       "  'veteran',\n",
       "  'day'],\n",
       " ['$', 'million', 'checkbook'],\n",
       " ['love', 'vet', 'say']]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the first five sentences normalized for the first article\n",
    "train_articles_normalized2[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_articles_normalized2 = np.array(train_articles_normalized2, dtype=\"object\")\n",
    "test_articles_normalized2 = np.array(test_articles_normalized2, dtype=\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters for the Gensim models\n",
    "window_context = 10\n",
    "min_word_count = 1\n",
    "sample = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This object generates document embeddings based on self-created word embeddings\n",
    "class GetDocumentEmbedding(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, modeltype, feature_size):\n",
    "        self.modeltype = modeltype\n",
    "        self.feature_size = feature_size\n",
    "        self.vocabulary = None\n",
    "        self.model = None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self      \n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        \n",
    "        if self.modeltype == \"word2vec\":\n",
    "            corpus = X.tolist()\n",
    "            corpus = [sentence for article in corpus for sentence in article]\n",
    "            model = word2vec.Word2Vec(corpus, vector_size=self.feature_size, window=window_context, min_count=min_word_count, sample=sample)\n",
    "            self.model = model\n",
    "            vocabulary = set(model.wv.index_to_key)\n",
    "        \n",
    "        elif self.modeltype == \"fasttext\":\n",
    "            corpus = X.tolist()\n",
    "            corpus = [sentence for article in corpus for sentence in article]\n",
    "            model = FastText(corpus, vector_size=self.feature_size, window=window_context, min_count=min_word_count, sample=sample, sg=1)\n",
    "            self.model = model\n",
    "            vocabulary = set(model.wv.index_to_key)\n",
    "        else:\n",
    "            vocabulary = set()\n",
    "\n",
    "        self.vocabulary = vocabulary\n",
    "            \n",
    "        document_embeddings=[]\n",
    "        for document in X:\n",
    "            document_embedding = np.zeros((self.feature_size), dtype=\"float32\")\n",
    "            n_tokens = 0\n",
    "            for sentence in document:\n",
    "                for token in sentence:\n",
    "                    if token in vocabulary:\n",
    "                        document_embedding = np.add(document_embedding, self.model.wv[token])\n",
    "                        n_tokens += 1\n",
    "\n",
    "            if n_tokens > 0:\n",
    "                document_embedding = np.divide(document_embedding, n_tokens)\n",
    "\n",
    "            document_embeddings.append(document_embedding)\n",
    "        return document_embeddings        \n",
    "        \n",
    "    def transform(self, X, y=None):\n",
    "        \n",
    "        document_embeddings=[]\n",
    "        for document in X:\n",
    "            document_embedding = np.zeros((self.feature_size), dtype=\"float32\")\n",
    "            n_tokens = 0\n",
    "            for sentence in document:\n",
    "                for token in sentence:\n",
    "                    if token in self.vocabulary:\n",
    "                        document_embedding = np.add(document_embedding, self.model.wv[token])\n",
    "                        n_tokens += 1\n",
    "\n",
    "            if n_tokens > 0:\n",
    "                document_embedding = np.divide(document_embedding, n_tokens)\n",
    "\n",
    "            document_embeddings.append(document_embedding)\n",
    "        return document_embeddings \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipeline\n",
    "full_pipeline = Pipeline([\n",
    "    (\"embedding\", GetDocumentEmbedding(\"word2vec\", 50)),\n",
    "    (\"logreg\", LogisticRegression(max_iter=300))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create grid search\n",
    "param_grid = [{\"embedding__feature_size\": [50,100,250,500]}]\n",
    "\n",
    "grid_search = GridSearchCV(full_pipeline, param_grid, cv=5, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('embedding',\n",
       "                                        GetDocumentEmbedding(feature_size=50,\n",
       "                                                             modeltype='word2vec')),\n",
       "                                       ('logreg',\n",
       "                                        LogisticRegression(max_iter=300))]),\n",
       "             param_grid=[{'embedding__feature_size': [50, 100, 250, 500]}],\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(train_articles_normalized2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.86321747, 0.86569447, 0.86590001, 0.86383667])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy scores on validation sets for each of the feature sizes\n",
    "grid_search.cv_results_[\"mean_test_score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'embedding__feature_size': 250}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best model feature size\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best feature size is 100. Let's use this to predict on the test set and to evaluate how well the use of self-trained Word2Vec word embeddings performs compared with the pre-trained embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create final pipeline\n",
    "full_pipeline = Pipeline([\n",
    "    (\"embedding\", GetDocumentEmbedding(\"word2vec\", 100)),\n",
    "    (\"logreg\", LogisticRegression(max_iter=300))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('embedding',\n",
       "                 GetDocumentEmbedding(feature_size=100, modeltype='word2vec')),\n",
       "                ('logreg', LogisticRegression(max_iter=300))])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_pipeline.fit(train_articles_normalized2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create predictions on test set\n",
    "predictions = full_pipeline.predict(test_articles_normalized2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on the test set is 0.8646864686468647\n"
     ]
    }
   ],
   "source": [
    "print(\"The accuracy on the test set is {}\".format(accuracy_score(predictions, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is quite high. It is higher than using the pre-trained word embeddings. But traditional feature engineering still outperforms the use of self-trained Word2Vec word embeddings. Let's see whether the use of FastText makes a difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipeline\n",
    "full_pipeline = Pipeline([\n",
    "    (\"embedding\", GetDocumentEmbedding(\"fasttext\", 50)),\n",
    "    (\"logreg\", LogisticRegression(max_iter=300))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create grid search\n",
    "param_grid = [{\"embedding__feature_size\": [50,100,250,500]}]\n",
    "\n",
    "grid_search = GridSearchCV(full_pipeline, param_grid, cv=5, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('embedding',\n",
       "                                        GetDocumentEmbedding(feature_size=50,\n",
       "                                                             modeltype='fasttext')),\n",
       "                                       ('logreg',\n",
       "                                        LogisticRegression(max_iter=300))]),\n",
       "             param_grid=[{'embedding__feature_size': [50, 100, 250, 500]}],\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(train_articles_normalized2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.87023289, 0.8737406 , 0.8772466 , 0.87848457])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy scores on validation sets for each of the feature sizes\n",
    "grid_search.cv_results_[\"mean_test_score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'embedding__feature_size': 500}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best model feature size\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best feature size is 500. Let's use this to predict on the test set and to evaluate how well the use of self-trained FastText word embeddings performs compared with the pre-trained embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create final pipeline\n",
    "full_pipeline = Pipeline([\n",
    "    (\"embedding\", GetDocumentEmbedding(\"fasttext\", 500)),\n",
    "    (\"logreg\", LogisticRegression(max_iter=300))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('embedding',\n",
       "                 GetDocumentEmbedding(feature_size=500, modeltype='fasttext')),\n",
       "                ('logreg', LogisticRegression(max_iter=300))])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_pipeline.fit(train_articles_normalized2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = full_pipeline.predict(test_articles_normalized2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on the test set is 0.8853135313531353\n"
     ]
    }
   ],
   "source": [
    "print(\"The accuracy on the test set is {}\".format(accuracy_score(predictions, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is quite high. It is higher than when using the pre-trained FastText word embeddings. It is also higher than when using self-trained Word2Vec word embeddings. However, the traditional feature engineering methods still work best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document embeddings from BERT sentence embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final feature engineering method we are going to examine are BERT sentence embeddings. These are also embeddings, but they are generated with a different neural network model. One of the main advantages over the other embedding methods is that the model learns the context of a word based on the surrounding words. For example, under the older embedding models, the word \"bank\" would have the same meaning irrespective of the context. Under BERT however, the word \"bank\" could mean different things, depending on the context.\n",
    "\n",
    "Let's load the BERT model from TensorFlow Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load BERT model\n",
    "preprocess_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
    "encoder_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\"\n",
    "os.environ['TFHUB_CACHE_DIR'] = 'D:/tensorflow hub/tf_cache'\n",
    "bert_preprocess_model = hub.KerasLayer(preprocess_url)\n",
    "bert_model = hub.KerasLayer(encoder_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next few cells I am going to create BERT document embeddings from BERT sentence embeddings, by averaging the sentence embeddings over the documents. Then, I will estimate a logistic regression model with the document embeddings as input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to get BERT document embeddings  from BERT sentence embeddings\n",
    "\n",
    "sentence_tokenizer = nltk.sent_tokenize\n",
    "\n",
    "def get_document_bert_embedding(article):\n",
    "    # get rid of new line seperators\n",
    "    article = article.replace(\"\\n\", \"\")\n",
    "    # create sentences\n",
    "    sentences = sentence_tokenizer(article)\n",
    "    # preprocess sentences\n",
    "    text_preprocessed = bert_preprocess_model(sentences)\n",
    "    # get sentence embeddings\n",
    "    bert_results = bert_model(text_preprocessed)\n",
    "    # get document embeddings\n",
    "    document_embedding = tf.reduce_mean(bert_results[\"pooled_output\"], 0).numpy()\n",
    "    return document_embedding   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear up memory\n",
    "del full_pipeline, predictions, grid_search, train_articles_normalized2, test_articles_normalized2, logreg_fasttext_pretrained, predictions_logreg_fasttext_pretrained, train_document_embeddings, test_document_embeddings, model, logreg_word2vec_pretrained, predictions_logreg_word2vec_pretrained, logreg_tfidf, predictions_logreg_tfidf, tfidf_test, tfidf_train, predictions_logreg_cv2, logreg_cv2, cv2_test, cv2_train, predictions_logreg_cv, logreg_cv, cv_test, cv_train, cv_odds, vocabulary, train_articles_normalized, test_articles_normalized, nlp, X_train, X_test, y, X, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get BERT document embeddings\n",
    "train_articles_document_bert_embeddings = [get_document_bert_embedding(article) for article in train_articles_list]\n",
    "test_articles_document_bert_embeddings = [get_document_bert_embedding(article) for article in test_articles_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_articles_document_bert_embeddings = np.array(train_articles_document_bert_embeddings)\n",
    "test_articles_document_bert_embeddings = np.array(test_articles_document_bert_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit logistic regression model with BERT document embeddings\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "logreg.fit(train_articles_document_bert_embeddings, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on the test set is 0.9240924092409241\n"
     ]
    }
   ],
   "source": [
    "# get predictions and print accuracy\n",
    "predictions = logreg.predict(test_articles_document_bert_embeddings)\n",
    "print(\"The accuracy on the test set is {}\".format(accuracy_score(predictions, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The document embeddings from the BERT sentence embeddings generate the highest accuracy on the test set!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project I classified fake news from real news with a logistic regression model, but with different feature engineering techniques, such as the Bag of Words Model, the TF-IDF Model, and document embeddings resulting from several word embedding techniques and the BERT sentence embedding model. Traditional feature engineering techniques, such as the Bag of Words Model seem to work very well on this data. However, the use of BERT sentence embeddings generate the best predictions on the test set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
